{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48a31662",
   "metadata": {},
   "source": [
    "##### Importing necessary Libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62849616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing all necessary libraries.\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import math\n",
    "import scipy\n",
    "import pprint\n",
    "\n",
    "# import scikit-learn as sk\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fa7e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetching the dataset (csv file).\n",
    "\n",
    "df = pd.read_csv('E:\\\\Govind_Work_Folder\\\\Career247_(Data_Science_Course)\\\\03. Data Science Course (Career247)\\\\Module 4 (Machine Learning)\\\\DecisionTreesImplementaion\\\\DecisionTreesFoundations\\\\breast_cancer_data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ba64c3",
   "metadata": {},
   "source": [
    "##### Step 1: EDA (Exploratory Data Analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5418f816",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Check basic informations.\n",
    "\n",
    "print(\"Shape:\", df.shape)\n",
    "print(\"*\" * 80)\n",
    "\n",
    "print(\"Columns:\", df.columns)\n",
    "print(\"*\" * 80)\n",
    "\n",
    "print(df.info())\n",
    "print(\"*\" * 80)\n",
    "\n",
    "print(df.describe())\n",
    "print(\"*\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7e5bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Check for missing values.\n",
    "\n",
    "print(\"Null Value Check:\", df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa32394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Correlation matrix.\n",
    "\n",
    "print(\"Correlation Matrix:\")\n",
    "# print(df.corr())\n",
    "print(df.corr(numeric_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655e7499",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Visualize the data.\n",
    "\n",
    "sns.heatmap(df.corr(numeric_only=True), annot=True, fmt=\".2f\", cmap=\"coolwarm\")\n",
    "plt.title(\"Correlation Matrix Heatmap\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0690285",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data for line plot.\n",
    "\n",
    "df10 = pd.DataFrame({\n",
    "    \"x-axis\": [1, 2, 3, 4, 5],\n",
    "    \"y-axis\": [10, 15, 13, 17, 20]\n",
    "})\n",
    "\n",
    "# Create line plot\n",
    "\n",
    "fig = px.line(df10, x=\"x-axis\", y=\"y-axis\", title=\"Simple Line Plot (Using Plotly Express)\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806c674f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data for scatter plot with categories.\n",
    "\n",
    "df20 = pd.DataFrame({\n",
    "    \"x\": [1, 2, 3, 4, 5, 6],\n",
    "    \"y\": [10, 14, 12, 18, 22, 19],\n",
    "    \"category\": [\"A\", \"B\", \"A\", \"B\", \"A\", \"B\"]\n",
    "})\n",
    "\n",
    "# Create scatter plot\n",
    "\n",
    "fig = px.scatter(df20, x=\"x\", y=\"y\", color=\"category\", size=\"y\", title=\"Scatter Plot with Categories (Using Plotly Express)\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06960330",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deleting columns:\n",
    "\n",
    "print(\"Dropping the redundant\\n\")\n",
    "df.drop(columns = ['id', 'Unnamed: 32'], axis = 1, inplace = True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d596655e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Unique value of Diagnosis column in the output label: \\n\")\n",
    "print(df['diagnosis'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8dbf47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output label / Target variable / Y-label : data distribution \n",
    "# pie-plot : proportion of M v/s B\n",
    "\n",
    "px.pie(df, \n",
    "       'diagnosis',\n",
    "       color = 'diagnosis',\n",
    "       color_discrete_sequence = ['#007500','#5CFF5C'],\n",
    "       title = \"Data Distribution\")\n",
    "\n",
    "# Inferences :\n",
    "# dataset is imbalanced (M : B = 63:37).\n",
    "# there are more cases of benign tumors than malignant tumors.\n",
    "# for imbalanced datasets, accuracy can be a misleading metric.\n",
    "# for example, if 90% of the cases are benign, the model will always predict \"benign\".\n",
    "# in such cases, we need \"Balanced accuracy\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc45cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visually compare the distribution of each feature.\n",
    "# for malignant tumours versus bening.\n",
    "# for a given feature, do its values tend to be different for malignant vs benign cases.\n",
    "\n",
    "for column in df.drop(\"diagnosis\", axis = 1).columns[:5]:\n",
    "\n",
    "    # for loop auto iterates through the first five feature columns in the dataframe.\n",
    "\n",
    "    fig = px.box(data_frame = df,\n",
    "                 x = 'diagnosis',\n",
    "                 color = 'diagnosis',\n",
    "                 y = column,\n",
    "                 color_discrete_sequence = ['#007500','#5CFF5C'],\n",
    "                 orientation = 'v')\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a28131c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in df.drop(\"diagnosis\",axis=1).columns[5:10]:\n",
    "    \n",
    "    # for loop auto iterates through the first five feature columns in the dataframe\n",
    "    fig = px.scatter(data_frame =df ,\n",
    "                 x=column,\n",
    "                 color = 'diagnosis',\n",
    "                 color_discrete_sequence = ['#007500','#5CFF5C'],\n",
    "                 orientation = 'v')\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb0ef43",
   "metadata": {},
   "source": [
    "##### Step 2: Creating co-relation with the targer variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042311e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# diagnosis : M or B :categorical\n",
    "# encode : 1 or 0 :categorical\n",
    "\n",
    "# this line converts the categorical feature into numerical\n",
    "df['diagnosis'] = (df['diagnosis'] == 'M').astype(int)\n",
    "\n",
    "# setting M = 1  then B = 0\n",
    "\n",
    "# take the correlation\n",
    "corr = df.corr()\n",
    "plt.figure(figsize = (20,20))\n",
    "\n",
    "# heatmap \n",
    "sns.heatmap(corr , cmap = 'viridis_r' , annot = True)\n",
    "\n",
    "plt.show()\n",
    "# correlation goes between : -1 to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc095f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also use it for corelation without chart.\n",
    "df.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91175898",
   "metadata": {},
   "source": [
    "##### Step 3: Feature Selection (Feature Engineering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc42bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We should now choose which features are good enough predictors to be used to train the model \n",
    "# get the absoulte correlation \n",
    "\n",
    "# select better correlated features\n",
    "# this is the filtering step\n",
    "# it creates a new list of relevant features\n",
    "cor_target = abs(corr['diagnosis'])\n",
    "\n",
    "# 0.25 is user defined. It is the hyper-parameter value\n",
    "relevant_features = cor_target[cor_target > 0.25]\n",
    "\n",
    "# collect the names of features\n",
    "# list comprehension\n",
    "\n",
    "names = [index for index,value in relevant_features.items()]\n",
    "\n",
    "# Drop the target variable from the results\n",
    "names.remove(\"diagnosis\")\n",
    "\n",
    "pprint.pprint(names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50aeda6a",
   "metadata": {},
   "source": [
    "##### Step 4: Assign Training Data and Training Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecaa69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[names].values\n",
    "\n",
    "# this line creates target vector or a target label \n",
    "# df['diagnosis'].values : (569, 1)\n",
    "y = df['diagnosis'].values.reshape(-1, 1)\n",
    "\n",
    "print(\"Input features are:\", X.shape,\"Output Label shape is: \", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febafb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to scale\n",
    "# Standardize / Z-score normalization\n",
    "# apply on X\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def scale(X):\n",
    "    '''\n",
    "    Parameters : numpy.ndarray) \n",
    "    Returns : numpy.ndarray\n",
    "    '''\n",
    "\n",
    "    # Compute the mean and standard deviation\n",
    "    mean = np.mean(X, axis=0)\n",
    "    std = np.std(X, axis=0)\n",
    "\n",
    "    # Standardize this data\n",
    "    X = (X - mean) / std\n",
    "\n",
    "    return X\n",
    "\n",
    "X = scale(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939bb491",
   "metadata": {},
   "source": [
    "##### Step 5: Model Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf2ae4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will start with all the examples at the Root Nodes\n",
    "# Then we will calculate the Information Gain for each feature / Gini Index for each feature\n",
    "# then we will pick the feature with the highest Information Gain / Gini Index\n",
    "# then we will split the data according to selected feature\n",
    "# we will repeat this process until we reach the stopping criteria\n",
    "\n",
    "# Node Class\n",
    "class Node:\n",
    "    '''\n",
    "    A class representing a Node in a Decision Tree.\n",
    "    '''\n",
    "    def __init__(self, feature=None, threshold=None, left=None, right=None,gain = None, value=None):\n",
    "        '''\n",
    "        Initializes a Node. \n",
    "\n",
    "        Parameters:\n",
    "        - feature: The index of the feature to split on.\n",
    "        - threshold: The threshold value for the split. Defaults to None\n",
    "        - left: The left child Node. Defaults to None\n",
    "        - right: The right child Node. Defaults to None\n",
    "        - value: The class label if it's a leaf node.\n",
    "        '''\n",
    "      \n",
    "        self.feature = feature\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.gain = gain\n",
    "        self.value = value\n",
    "\n",
    "'''\n",
    "Explanation : \n",
    "self.threshold = threshold\n",
    "self.feature = feature \n",
    "The above two are used by Decision Nodes.\n",
    "They store the question being asked at this node .\n",
    "For example , \"Is the radius_mean < 15.5 ? \"\n",
    "\n",
    "self.left = left and self.right = right\n",
    "Used by decision nodes to point to the left and right child nodes.  \n",
    "They are also called pointer nodes.\n",
    "       \n",
    "self.value = value\n",
    "used  by leaf nodes to store the class label.\n",
    "If a node is a final endpoint . it does not ask any questions\n",
    "it holds predicted class label or prediciton for each branch\n",
    "self.value will be 0(Benign) or 1(Malignant) for leaf nodes.\n",
    "\n",
    "self.gain = gain\n",
    "Used by Decision Nodes to store the Information Gain or Gini Index of the split.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c5a1cd",
   "metadata": {},
   "source": [
    "##### Step 6: Building the Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "129130aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree:\n",
    "    '''\n",
    "    This is a decision tree classifier.\n",
    "    '''  \n",
    "\n",
    "    def __init__(self,min_samples = 2 , max_depth = 3):\n",
    "        self.min_samples = min_samples\n",
    "        self.max_depth = max_depth\n",
    "    \n",
    "        \"We are setting hyper-parameters to control the growth of the tree prevent overfitting\"\n",
    "        \n",
    "    def split_data(self, dataset,feature, threshold):\n",
    "        '''\n",
    "        Splits the given dataset based on the feature and threshold.\n",
    "        parameters:\n",
    "        - dataset: The dataset to split.\n",
    "        - feature  : Index of the feature to split on.\n",
    "        - threshold: The threshold value for the split.\n",
    "    \n",
    "        Returns : \n",
    "        left_dataset : subset of data with values less than or equal to the threshold\n",
    "        right_dataset : subset of data with values greater than the threshold\n",
    "'''  \n",
    "        # create empty arrays\n",
    "        left_dataset = []\n",
    "        right_dataset = []\n",
    "      \n",
    "        # loop through each row in the dataset in left and right basis the feature and threshold\n",
    "   \n",
    "        for row in dataset:\n",
    "            if row[feature] <= threshold:\n",
    "                left_dataset.append(row)\n",
    "            else:\n",
    "                right_dataset.append(row)\n",
    "               \n",
    "        # convert the left and right datasets into numpy arrays\n",
    "        left_dataset = np.array(left_dataset)\n",
    "        right_dataset = np.array(right_dataset)\n",
    "     \n",
    "        return left_dataset, right_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389e0bf5",
   "metadata": {},
   "source": [
    "##### Step 7: Entropy calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15cb7db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write function to calculate Entropy\n",
    "\n",
    "def entropy(self, y):\n",
    "        '''\n",
    "        Computes the entropy for given labels\n",
    "        Entropy suggests impurity or disorder in the dataset.\n",
    "\n",
    "        Returns : float : Entropy value\n",
    "        '''\n",
    "\n",
    "        entropy = 0.0\n",
    "        # this initializes the entropy to zero\n",
    "\n",
    "        # use numpy's unique function to get the unique labels in y\n",
    "        labels = np.unique(y)\n",
    "\n",
    "        for label in labels:\n",
    "            # find examples in y that have the current label\n",
    "            label_examples = y[y == label]\n",
    "\n",
    "            # Calculate the ratio of current label in y\n",
    "            pl = len(label_examples) / len(y)\n",
    "\n",
    "            # calculate the entropy for the current label and ratio\n",
    "            entropy += -pl * np.log2(pl)\n",
    "\n",
    "        return entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0c011f",
   "metadata": {},
   "source": [
    "##### Step 8: Gini Index / Information Gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ff10241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write function to calculate Gini Index/Information Gain\n",
    "   \n",
    "def information_gain(self,parent,left,right):\n",
    "        '''\n",
    "        Computes the information gain from splitting the parent dataset into two datasets\n",
    "\n",
    "        Parameters:\n",
    "        parent(ndarray): Input parent dataset\n",
    "        left: subset of parent dataset after the split on the feature\n",
    "        right: subset of parent dataset after the split on the feature\n",
    "       \n",
    "        Returns: \n",
    "        Information Gain on the split: float\n",
    "        '''\n",
    "\n",
    "# intiialize the information gain to zero\n",
    "        information_gain = 0.0\n",
    "        \n",
    "        # compute the entropy of the parent dataset\n",
    "        parent_entropy = self.entropy(parent)\n",
    "        \n",
    "        # calculate the weights for left and right datasets/nodes\n",
    "        weight_left = len(left) / len(parent)\n",
    "        weight_right = len(right) / len(parent)\n",
    "        \n",
    "        # compute the entropy of the left and right datasets/nodes\n",
    "        entropy_left,entropy_right = self.entropy(left) , self.entropy(right)\n",
    "        \n",
    "        # calculate the weighted entropy \n",
    "        weighted_entropy = (weight_left * entropy_left) + (weight_right * entropy_right)\n",
    "        \n",
    "        # calculate the information gain\n",
    "        information_gain = parent_entropy - weighted_entropy\n",
    "\n",
    "        return information_gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de402e68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
